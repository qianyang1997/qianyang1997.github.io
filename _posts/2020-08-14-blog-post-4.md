---
title: 'Thoughts on Bias Detection'
date: 2020-12-01
permalink: /posts/2020-12-01/
tags:
  - cool posts
  - category1
  - category2
---

Lately, I've been contemplating about starting a project on fake news detection using data sources from Twitter and major news outlets. As with any research endeavor, the initial steps towards problem definition are most difficult. Here, I want to briefly talk about the thoughts behind this project and the challenges presented. As I'm writing this post, my thoughts are still quite unstructured, but overtime, I'm confident the overarching scope will become clearer to me. 

Fake news detection is a hot area of data science in the private and public sectors. Almost every major news outlet or government-funded media research institution has invested in it. The idea at the very top level is not too sophisticated; all that's needed is a machine learning algorithm that builds a classification or regression model which categorizes a specific piece of textual content from Twitter or Facebook as "real" or "fabricated". One issue of this approach, however, is labeling &ndash; what reference frame should we rely upon to classify news articles into different buckets in the first place? It is fairly common to bypass the task of labeling by using known  datasets such as FakeNewsNet, where tens of thousands of Tweets or Buzzfeed news are already labeled. However, using an established dataset without questioning its reliability is quite against the purpose of the project, as my goal is to investigate textual evidence without assumptions. Note that it does not mean I will be building my own dataset from scratch &ndash; I simply want to be mindful of the hypotheses that the project relies on.

Before I discuss my vision for this project, I want to outline some of the challenges related to the traditional approach of fake news detection.

First, as we all may have experienced, classifying fake news is far from a black-and-white task. Oftentimes facts and fabricated claims are separated by a very fine line. It's quite hard to find a piece of Tweet or news that has absolutely no truth in it &ndash; even radical opinions are comprised of some elements of truth (and this, of course, is what makes the intersection of linguistics and data science so interesting). 

In addition, the linguistic structures of what we colloquially define as "fake news" &ndash; news with fabricated information built intentionally to deceive &ndash; are so varied that it's quite difficult to establish uniform features for hard classification. 

Furthermore, as we build natural language processing pipelines, our focus is on what's being written rather than what's not. We all write stories that serve our bias; even the most matter-of-fact style of reporting could omit evidence that counters its narrative. Solely relying on text-based data would lead us to overlook what's held back.

Finally, there's an inherent difference between disinformation, which is intentionally deceptive, and misinformation, which encompasses all types of false or inaccurate information. Machine learning algorithms may be able to detect specific rhetorical or semantic structures to differentiate facts from fiction, but the task is more complicated when the authors of falsified news believe in those themselves. To overcome this challenge, we will need to deal with the presentation of "intention" as well as that of "bias", which is very subjective and hard to quantify.

This last point of consideration, on the other hand, opened up a new path for me to explore. Rather than developing hard classification rules for falsified news, I find bias detection to be a much more interesting (and ethically grounded) topic. For one, bias is universally present as long as our species is still around. Extreme bias could be thought of as a particular subset of misinformation. The data sources for my project in question are abundant; even reputable news outlets communicate subjective opinions that could unintentionally lead to misunderstanding and polarization. Futhermore, I consider bias detection to be a more "ethical" project for me to play around with, partly due to my belief that it's impossible to develop a reference frame for the degree of truth in a piece of text. Besides, I'm not entirely comfortable with the idea of "imposing" any sort of hard rules on something grounded in subjectivity. 

For my project, I hope to quantify our perception of bias in national media by analyzing the semantic and rhetorical structures of news articles from different web sources. Establishing features and performing stratified sampling will be the bulk of this project. After setting up the pipeline, I will implement a model on a continuous scale. Once this model produces insightful results, I will investigate real-life use cases for my findings, including a more refined definition to the idea of misinformation. Ultimately, I hope this project could inform some of the cultural and philosophical topics we've been struggling with as a 21st-century society founded on 18th-century ideologies &dash; for instance, the danger of polarized political rhetoric and the never-ending debate of discourse competition vs. free speech.


